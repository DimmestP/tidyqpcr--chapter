@article{Li2020,
   abstract = {In December 2019, a new coronavirus disease (COVID-19) outbreak occurred in Wuhan, China. Severe acute respiratory syndrome-coronavirus-2 (SARS-CoV-2), which is the seventh coronavirus known to infect humans, is highly contagious and has rapidly expanded worldwide since its discovery. Quantitative nucleic acid testing has become the gold standard for diagnosis and guiding clinical decisions regarding the use of antiviral therapy. However, the RT-qPCR assays targeting SARS-CoV-2 have a number of challenges, especially in terms of primer design. Primers are the pivotal components of a RT-qPCR assay. Once virus mutation and recombination occur, it is difficult to effectively diagnose viral infection by existing RT-qPCR primers. Some primers and probes have also been made available on the WHO website for reference. However, no previous review has systematically compared the previously reported primers and probes and described how to design new primers in the event of a new coronavirus infection. This review focuses on how primers and probes can be designed methodically and rationally, and how the sensitivity and specificity of the detection process can be improved. This brief review will be useful for the accurate diagnosis and timely treatment of the new coronavirus pneumonia.},
   author = {Dandan Li and Jiawei Zhang and Jinming Li},
   doi = {10.7150/THNO.47649},
   issn = {18387640},
   issue = {16},
   journal = {Theranostics},
   keywords = {Coronavirus,Primer design,Quantitative nucleic acid testing,SARS-CoV-2,Sensitivity},
   pages = {7150-7162},
   pmid = {32641984},
   publisher = {Ivyspring International Publisher},
   title = {Primer design for quantitative real-time PCR for the emerging Coronavirus SARS-CoV-2},
   volume = {10},
   year = {2020},
}
@article{Bustin2021,
   abstract = {Although molecular testing, and RT-qPCR in particular, has been an indispensable component in the scientific armoury targeting SARS-CoV-2, there are numerous falsehoods, misconceptions, assumptions and exaggerated expectations with regards to capability, performance and usefulness of the technology. It is essential that the true strengths and limitations, although publicised for at least twenty years, are restated in the context of the current COVID-19 epidemic. The main objective of this commentary is to address and help stop the unfounded and debilitating speculation surrounding its use.},
   author = {Stephen Bustin and Reinhold Mueller and Gregory Shipley and Tania Nolan},
   doi = {10.3390/IJMS22052459},
   issn = {1422-0067},
   issue = {5},
   journal = {International Journal of Molecular Sciences 2021, Vol. 22, Page 2459},
   keywords = {19,2,COVID,CoV,RT,SARS,molecular diagnostics,qPCR,quantification cycle},
   month = {2},
   pages = {2459},
   pmid = {33671091},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {COVID-19 and Diagnostic Testing for SARS-CoV-2 by RT-qPCR—Facts and Fallacies},
   volume = {22},
   url = {https://www.mdpi.com/1422-0067/22/5/2459/htm https://www.mdpi.com/1422-0067/22/5/2459},
   year = {2021},
}
@article{Tajadini2014,
   abstract = {BACKGROUND: Progesterone is a steroid hormone that modulates proliferation and differentiation in a cell phase and tissue-specific manner. Its function in breast cancer cells is of great significance since it can predict susceptibility of tumor cells to inhibitory effects of progesterone as adjuvant therapy.\n\nMATERIALS AND METHODS: Stable clones overexpressing cyclin E (EL) and its low molecular weight isoforms (LMW-Es) were generated and treated with various concentrations of progesterone. Cell proliferation was assessed 24 and 48 h after the treatment. Changes in progesterone receptor (PR) expression were measured by real-time polymerase chain reaction.\n\nRESULTS: Here we demonstrated that overexpression of EL and LMW-Es have divergent effects with regard to progesterone response. We found that progesterone could significantly decrease the growth rate of EL-expressing cells in the second cell cycle after treatment; however, progesterone was ineffective to arrest growth of LMW-Es expressing cells. PR expression level was at control level in EL-expressing cells but was downregulatedin LMW-Esexpressing clones.\n\nCONCLUSION: These results were in line with progesterone response of studied cells. The drop in PR expression together with altered distribution of p21 and p27 can explain different effects of cyclin E isoforms expression on progesterone responsivity. These data bring cyclin E status of cancer cells as a marker for predicting the efficacy of progesterone treatment.},
   author = {Mohamadhasan Tajadini and Mojtaba Panjehpour and Shaghayegh Haghjooy Javanmard},
   doi = {10.4103/2277-9175.127998},
   issn = {2277-9175},
   issue = {1},
   journal = {Advanced Biomedical Research},
   keywords = {Quantitative real-time polymerase chain reaction,SYBR Green,TaqMan},
   pages = {85},
   pmid = {24761393},
   publisher = {Wolters Kluwer -- Medknow Publications},
   title = {Comparison of SYBR Green and TaqMan methods in quantitative real-time polymerase chain reaction analysis of four adenosine receptor subtypes},
   volume = {3},
   url = {/pmc/articles/PMC3988599/ /pmc/articles/PMC3988599/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3988599/},
   year = {2014},
}
@article{Holland1991,
   abstract = {The 5′ → 3′ exonuclease activity of the thermostable enzyme Thermus aquatic us DNA polymerase may be employed in a polymerase chain reaction product detection system to generate a specific detectable signal concomitantly with amplification. An oligonucleotide probe, nonextendable at the 3′ end, labeled at the 5′ end, and designed to hybridize within the target sequence, is introduced into the polymerase chain reaction assay. Annealing of probe to one of the polymerase chain reaction product strands during the course of amplification generates a substrate suitable for exonuclease activity. During amplification, the 5′ → 3′ exonuclease activity of T. aquaticus DNA polymerase degrades the probe into smaller fragments that can be differentiated from undegraded probe. The assay is sensitive and specific and is a significant improvement over more cumbersome detection methods.},
   author = {Pamela M. Holland and Richard D. Abramson and Robert Watson and David H. Gelfand},
   doi = {10.1073/PNAS.88.16.7276},
   issn = {00278424},
   issue = {16},
   journal = {Proceedings of the National Academy of Sciences of the United States of America},
   keywords = {Human immunodeficiency virus,Oligonucleotide probe},
   pages = {7276-7280},
   pmid = {1871133},
   publisher = {National Academy of Sciences},
   title = {Detection of specific polymerase chain reaction product by utilizing the 5′ → 3′ exonuclease activity of Thermus aquaticus DNA polymerase},
   volume = {88},
   url = {https://www.pnas.org},
   year = {1991},
}
@article{Schindler2022,
   abstract = {Science across all disciplines has become increasingly data-driven, leading to additional needs with respect to software for collecting, processing and analysingdata. Thus, transparency about software used as part of the scientific process iscrucial to understand provenance of individual research data and insights, is aprerequisite for reproducibility and can enable macro-analysis of the evolution ofscientific methods over time. However, missing rigor in software citation practicesrenders the automated detection and disambiguation of software mentions achallenging problem. In this work, we provide a large-scale analysis of software usageand citation practices facilitated through an unprecedented knowledge graph ofsoftware mentions and affiliated metadata generated through supervised informationextraction models trained on a unique gold standard corpus and applied to more than3 million scientific articles. Our information extraction approach distinguishesdifferent types of software and mentions, disambiguates mentions and outperformsthe state-of-the-art significantly, leading to the most comprehensive corpus of 11.8Msoftware mentions that are described through a knowledge graph consisting of morethan 300 M triples. Our analysis provides insights into the evolution of softwareusage and citation patterns across various fields, ranks of journals, and impact ofpublications. Whereas, to the best of our knowledge, this is the most comprehensiveanalysis of software use and citation at the time, all data and models are sharedpublicly to facilitate further research into scientific use and citation of software},
   author = {David Schindler and Felix Bensmann and Stefan Dietze and Frank Krüger},
   doi = {10.7717/PEERJ-CS.835/SUPP-1},
   issn = {23765992},
   journal = {PeerJ Computer Science},
   keywords = {Knowledge graph,Named entity recognition,Software citation,Software mention,Subjects Data Mining and Machine Learning,World Wide Web and Web Science Keywords Knowledge graph},
   month = {1},
   pages = {e835},
   publisher = {PeerJ},
   title = {The role of software in science: a knowledge graph-based analysis of software mentions in PubMed Central},
   volume = {8},
   url = {https://peerj.com/articles/cs-835},
   year = {2022},
}
@article{Prause2010,
   abstract = {The EU subsidizes research projects in the ICT area with hundreds of millions of Euros per year with the aim of strengthening Europe's global competitiveness. A key requirement of EU projects is the involvement of partners from at least three different countries. This leads to highly distributed software environments where company, country, and culture boundaries run in the midst of tasks like requirements engineering, architectural design, implementation or testing. We present results from an empirical study involving more than 50 transnational, multimillion Euro projects of the Sixth Framework Programme. The results show which tools are accepted by developers and used in practice in the respective phases of the software process. Finally, we shape the idea of Research Software Engineering. © 2010 IEEE.},
   author = {Christian R. Prause and René Reiners and Silviya Dencheva},
   doi = {10.1109/ICGSE.2010.13},
   isbn = {9780769541228},
   journal = {Proceedings - 5th International Conference on Global Software Engineering, ICGSE 2010},
   pages = {23-32},
   publisher = {IEEE Computer Society},
   title = {Empirical study of tool support in highly distributed research projects},
   year = {2010},
}
@article{Mallona2017,
   abstract = {Chainy is a cross-platform web tool providing systematic pipelines and steady criteria to process real-time PCR data, including the calculation of efficiencies from raw data by kinetic methods, evaluation of the suitability of multiple references, standardized normalization using one or more references, and group-wise relative quantification statistical testing. We illustrate the utility of Chainy for differential expression and chromatin immunoprecipitation enrichment (ChIP-QPCR) analysis.},
   author = {Izaskun Mallona and Anna Díez-Villanueva and Berta Martín and Miguel A. Peinado},
   doi = {10.1093/BIOINFORMATICS/BTW839},
   issn = {1367-4803},
   issue = {9},
   journal = {Bioinformatics},
   month = {5},
   pages = {1411-1413},
   pmid = {28453678},
   publisher = {Oxford Academic},
   title = {Chainy: an universal tool for standardized relative quantification in real-time PCR},
   volume = {33},
   url = {https://academic.oup.com/bioinformatics/article/33/9/1411/2840141},
   year = {2017},
}
@article{Forward2002,
   abstract = {This paper highlights the results of a survey of software professionals. One of the goals of this survey was to uncover the perceived relevance (or lack thereof) of software documentation, and the tools and technologies used to maintain, verify and validate such documents. The survey results highlight the preferences for and aversions against software documentation tools. Participants agree that documentation tools should seek to better extract knowledge from core resources. These resources include the system's source code, test code and changes to both. Resulting technologies could then help reduce the effort required for documentation maintenance, something that is shown to rarely occur. Our data reports compelling evidence that software professionals value technologies that improve automation of the documentation process, as well as facilitating its maintenance.},
   author = {Andrew Forward and Timothy C. Lethbridge},
   city = {New York, New York, USA},
   doi = {10.1145/585058.585065},
   isbn = {1581135947},
   journal = {Proceedings of the 2002 ACM symposium on Document engineering  - DocEng '02},
   keywords = {D27 [Distribution,Experimentation,Human Factors,Maintenance,Measurement Keywords Software documentation,and Enhancement]: Docu-mentation General Terms Documentation,documentation relevance,documentation survey,documentation technologies,program comprehension,software engineering,software maintenance},
   pages = {26},
   publisher = {ACM Press},
   title = {The relevance of software documentation, tools and technologies},
   url = {http://portal.acm.org/citation.cfm?doid=585058.585065},
   year = {2002},
}
@article{Aghajani2019,
   abstract = {(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavourable take on documentation. Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.},
   author = {Emad Aghajani and Csaba Nagy and Olga Lucero Vega-Marquez and Mario Linares-Vasquez and Laura Moreno and Gabriele Bavota and Michele Lanza},
   doi = {10.1109/ICSE.2019.00122},
   isbn = {9781728108698},
   issn = {02705257},
   journal = {Proceedings - International Conference on Software Engineering},
   keywords = {Documentation,Empirical Study},
   month = {5},
   pages = {1199-1210},
   publisher = {IEEE Computer Society},
   title = {Software Documentation Issues Unveiled},
   volume = {2019-May},
   year = {2019},
}
@article{Geiger2018,
   abstract = {Computational research and data analytics increasingly relies on complex ecosystems of open source software (OSS) “libraries” – curated collections of reusable code that programmers import to perform a specific task. Software documentation for these libraries is crucial in helping programmers/analysts know what libraries are available and how to use them. Yet documentation for open source software libraries is widely considered low-quality. This article is a collaboration between CSCW researchers and contributors to data analytics OSS libraries, based on ethnographic fieldwork and qualitative interviews. We examine several issues around the formats, practices, and challenges around documentation in these largely volunteer-based projects. There are many different kinds and formats of documentation that exist around such libraries, which play a variety of educational, promotional, and organizational roles. The work behind documentation is similarly multifaceted, including writing, reviewing, maintaining, and organizing documentation. Different aspects of documentation work require contributors to have different sets of skills and overcome various social and technical barriers. Finally, most of our interviewees do not report high levels of intrinsic enjoyment for doing documentation work (compared to writing code). Their motivation is affected by personal and project-specific factors, such as the perceived level of credit for doing documentation work versus more ‘technical’ tasks like adding new features or fixing bugs. In studying documentation work for data analytics OSS libraries, we gain a new window into the changing practices of data-intensive research, as well as help practitioners better understand how to support this often invisible and infrastructural work in their projects.},
   author = {R. Stuart Geiger and Nelle Varoquaux and Charlotte Mazel-Cabasse and Chris Holdgraf},
   doi = {10.1007/S10606-018-9333-1/TABLES/2},
   issn = {15737551},
   issue = {3-6},
   journal = {Computer Supported Cooperative Work: CSCW: An International Journal},
   keywords = {Collaboration,Documentation,Ethnography,Infrastructure,Invisible work,Motivations,Open source,Peer production,Standards},
   month = {12},
   pages = {767-802},
   publisher = {Springer Netherlands},
   title = {The Types, Roles, and Practices of Documentation in Data Analytics Open Source Software Libraries: A Collaborative Ethnography of Documentation Work},
   volume = {27},
   url = {https://link.springer.com/article/10.1007/s10606-018-9333-1},
   year = {2018},
}
@article{ValeroMora2012,
   abstract = {Since R was first launched, it has managed to gain the support of an ever-increasing percentage of academic and professional statisticians. However, the spread of its use among novice and occasional users of statistics have not progressed at the same pace, which can be attributed partially to the lack of a graphical user interface (GUI). Nevertheless, this situation has changed in the last years and there is currently several projects that have added GUIs to R. This article discusses briefly the history of GUIs for data analysis and then introduces the papers submitted to an special issue of the Journal of Statistical Software on GUIs for R.},
   author = {Pedro M. Valero-Mora and Rubén D. Ledesma},
   doi = {10.18637/JSS.V049.I01},
   issn = {1548-7660},
   journal = {Journal of Statistical Software},
   keywords = {GUI,R,Statistical software},
   month = {6},
   pages = {1-8},
   publisher = {American Statistical Association},
   title = {Graphical User Interfaces for R},
   volume = {49},
   url = {https://www.jstatsoft.org/index.php/jss/article/view/v049i01},
   year = {2012},
}
@article{Staggers2000,
   abstract = {Despite the general adoption of graphical users interfaces (GUIs) in health care, few empirical data document the impact of this move on system users. This study compares two distinctly different user interfaces, a legacy text-based interface and a prototype graphical interface, for differences in nurses' response time (RT), errors, and satisfaction when the interfaces are used in the performance of computerized nursing order tasks. In a medical center on the East Coast of the United States, 98 randomly selected male and female nurses completed 40 tasks using each interface. Nurses completed four different types of order tasks (create, activate, modify, and discontinue). Using a repeated-measures and Latin square design, the study was counterbalanced for tasks, interface types, and blocks of trials. Overall, nurses had significantly faster response times (P < 0.0001) and fewer errors (P < 0.0001) using the prototype GUI than the text-based interface. The GUI was also rated significantly higher for satisfaction than the text system, and the GUI was faster to learn (P < 0.0001). Therefore, the results indicated that the use of a prototype GUI for nursing orders significantly enhances user performance and satisfaction. Consideration should be given to redesigning older user interfaces to create more modern ones by using human factors principles and input from user-centered focus groups. Future work should examine prospective nursing interfaces for highly complex interactions in computer-based patient records, detail the severity of errors made on line, and explore designs to optimize interactions in life-critical systems.},
   author = {Nancy Staggers and David Kobus},
   doi = {10.1136/JAMIA.2000.0070164},
   issn = {1067-5027},
   issue = {2},
   journal = {Journal of the American Medical Informatics Association : JAMIA},
   keywords = {Analysis of Variance,Attitude to Computers,Comparative Study,Computer Graphics*,Computerized*,Consumer Behavior*,D Kobus,Female,Humans,MEDLINE,Male,Medical Records Systems,N Staggers,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Nurses,Nursing Care / organization & administration*,PMC61470,PubMed Abstract,Research Support,Time Factors,User-Computer Interface*,doi:10.1136/jamia.2000.0070164,pmid:10730600},
   pages = {164-176},
   pmid = {10730600},
   publisher = {J Am Med Inform Assoc},
   title = {Comparing response time, errors, and satisfaction between text-based and graphical user interfaces during nursing order tasks},
   volume = {7},
   url = {https://pubmed.ncbi.nlm.nih.gov/10730600/},
   year = {2000},
}
@article{Unwin2012,
   abstract = {Graphical user interfaces (GUIs) are gradually becoming more powerful and more accepted. They are the standard way of interacting with the web and play an increasing role in many software applications. Nevertheless, they have not been generally adopted, and critics point to particular weaknesses and disadvantages. Many of these are due more to flaws in design and implementation than to the basic concepts of GUIs. More attention could be paid to what users want to do and how a GUI might be developed to support these goals. Using a dataset about Oscar nominees and winners, this paper considers what analyses statisticians might carry out and what kind of GUI would be appropriate for these tasks. (It also offers some insights into the Oscars dataset.)},
   author = {Antony Unwin},
   doi = {10.18637/JSS.V049.I11},
   issn = {1548-7660},
   journal = {Journal of Statistical Software},
   keywords = {GUI,Iplots,JMP,Mondrian,Oscars},
   month = {6},
   pages = {1-18},
   publisher = {American Statistical Association},
   title = {Oscars and Interfaces},
   volume = {49},
   url = {https://www.jstatsoft.org/index.php/jss/article/view/v049i11},
   year = {2012},
}
@article{Perkins2012,
   abstract = {Background: Measuring gene transcription using real-time reverse transcription polymerase chain reaction (RT-qPCR) technology is a mainstay of molecular biology. Technologies now exist to measure the abundance of many transcripts in parallel. The selection of the optimal reference gene for the normalisation of this data is a recurring problem, and several algorithms have been developed in order to solve it. So far nothing in R exists to unite these methods, together with other functions to read in and normalise the data using the chosen reference gene(s).Results: We have developed two R/Bioconductor packages, ReadqPCR and NormqPCR, intended for a user with some experience with high-throughput data analysis using R, who wishes to use R to analyse RT-qPCR data. We illustrate their potential use in a workflow analysing a generic RT-qPCR experiment, and apply this to a real dataset. Packages are available from http://www.bioconductor.org/packages/release/bioc/html/ReadqPCR.htmland http://www.bioconductor.org/packages/release/bioc/html/NormqPCR.html. Conclusions: These packages increase the repetoire of RT-qPCR analysis tools available to the R user and allow them to (amongst other things) read their data into R, hold it in an ExpressionSet compatible R object, choose appropriate reference genes, normalise the data and look for differential expression between samples. © 2012 Perkins et al.; licensee BioMed Central Ltd.},
   author = {James R. Perkins and John M. Dawes and Steve B. McMahon and David L.H. Bennett and Christine Orengo and Matthias Kohl},
   doi = {10.1186/1471-2164-13-296/FIGURES/4},
   issn = {14712164},
   issue = {1},
   journal = {BMC Genomics},
   keywords = {Animal Genetics and Genomics,Life Sciences,Microarrays,Microbial Genetics and Genomics,Plant Genetics and Genomics,Proteomics,general},
   month = {7},
   pages = {1-8},
   pmid = {22748112},
   publisher = {BioMed Central},
   title = {ReadqPCR and NormqPCR: R packages for the reading, quality checking and normalisation of RT-qPCR quantification cycle (Cq) data},
   volume = {13},
   url = {https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-13-296},
   year = {2012},
}
@article{Baebler2017,
   abstract = {Background: Quantitative molecular biology remains a challenge for researchers due to inconsistent approaches for control of errors in the final results. Due to several factors that can influence the final result, quantitative analysis and interpretation of qPCR data are still not trivial. Together with the development of high-throughput qPCR platforms, there is a need for a tool allowing for robust, reliable and fast nucleic acid quantification. Results: We have developed "quantGenius" ( http://quantgenius.nib.si ), an open-access web application for a reliable qPCR-based quantification of nucleic acids. The quantGenius workflow interactively guides the user through data import, quality control (QC) and calculation steps. The input is machine- and chemistry-independent. Quantification is performed using the standard curve approach, with normalization to one or several reference genes. The special feature of the application is the implementation of user-guided QC-based decision support system, based on qPCR standards, that takes into account pipetting errors, assay amplification efficiencies, limits of detection and quantification of the assays as well as the control of PCR inhibition in individual samples. The intermediate calculations and final results are exportable in a data matrix suitable for further statistical analysis or visualization. We additionally compare the most important features of quantGenius with similar advanced software tools and illustrate the importance of proper QC system in the analysis of qPCR data in two use cases. Conclusions: To our knowledge, quantGenius is the only qPCR data analysis tool that integrates QC-based decision support and will help scientists to obtain reliable results which are the basis for biologically meaningful data interpretation.},
   author = {Špela Baebler and Miha Svalina and Marko Petek and Katja Stare and Ana Rotter and Maruša Pompe-Novak and Kristina Gruden},
   doi = {10.1186/S12859-017-1688-7/FIGURES/5},
   issn = {14712105},
   issue = {1},
   journal = {BMC Bioinformatics},
   keywords = {Decision support system,Nucleic acid quantification,Quantitative PCR,Quantitative molecular biology,Web application},
   month = {5},
   pages = {1-11},
   pmid = {28545393},
   publisher = {BioMed Central Ltd.},
   title = {QuantGenius: Implementation of a decision support system for qPCR-based gene quantification},
   volume = {18},
   url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-017-1688-7},
   year = {2017},
}
@article{Feuer2015,
   abstract = {Background: Gene expression analysis is an essential part of biological and medical investigations. Quantitative real-time PCR (qPCR) is characterized with excellent sensitivity, dynamic range, reproducibility and is still regarded to be the gold standard for quantifying transcripts abundance. Parallelization of qPCR such as by microfluidic Taqman Fluidigm Biomark Platform enables evaluation of multiple transcripts in samples treated under various conditions. Despite advanced technologies, correct evaluation of the measurements remains challenging. Most widely used methods for evaluating or calculating gene expression data include geNorm and ΔΔCt, respectively. They rely on one or several stable reference genes (RGs) for normalization, thus potentially causing biased results. We therefore applied multivariable regression with a tailored error model to overcome the necessity of stable RGs. Results We developed a RG independent data normalization approach based on a tailored linear error model for parallel qPCR data, called LEMming. It uses the assumption that the mean Ct values within samples of similarly treated groups are equal. Performance of LEMming was evaluated in three data sets with different stability patterns of RGs and compared to the results of geNorm normalization. Data set 1 showed that both methods gave similar results if stable RGs are available. Data set 2 included RGs which are stable according to geNorm criteria, but became differentially expressed in normalized data evaluated by a t-test. geN-orm-normalized data showed an effect of a shifted mean per gene per condition whereas LEMming-normalized data did not. Comparing the decrease of standard deviation from raw data to geNorm and to LEMming, the latter was superior. In data set 3 according to geNorm calculated average expression stability and pairwise variation, stable RGs were available, but t-tests of raw data contradicted this. Normalization with RGs resulted in distorted data contradicting literature, while LEMming normalized data did not. Conclusions: If RGs are coexpressed but are not independent of the experimental conditions the stability criteria based on inter- and intragroup variation fail. The linear error model developed, LEMming, overcomes the dependency of using RGs for parallel qPCR measurements, besides resolving biases of both technical and biological nature in qPCR. However, to distinguish systematic errors per treated group from a global treatment effect an additional measurement is needed. Quantification of total cDNA content per sample helps to identify systematic errors.},
   author = {Ronny Feuer and Sebastian Vlaic and Janine Arlt and Oliver Sawodny and Uta Dahmen and Ulrich M. Zanger and Maria Thomas and Lars Kaderali},
   doi = {10.1371/JOURNAL.PONE.0135852},
   issn = {19326203},
   issue = {9},
   journal = {PLoS ONE},
   month = {9},
   pmid = {26325269},
   publisher = {Public Library of Science},
   title = {LEMming: A linear error model to normalize parallel quantitative real-time PCR (qPCR) data as an alternative to reference gene based methods},
   volume = {10},
   year = {2015},
}
@article{Rodiger2015,
   abstract = {There is an ever-increasing number of applications, which use quantitative PCR (qPCR) or digital PCR (dPCR) to elicit fundamentals of biological processes. Moreover, quantitative isothermal amplification (qIA) methods have become more prominent in life sciences and point-of-carediagnostics. Additionally, the analysis of melting data is essential during many experiments. Several software packages have been developed for the analysis of such datasets. In most cases, the software is either distributed as closed source software or as monolithic block with little freedom to perform highly customized analysis procedures. We argue, among others, that R is an excellent foundation for reproducible and transparent data analysis in a highly customizable cross-platform environment. However, for novices it is often challenging to master R or learn capabilities of the vast number of packages available. In the paper, we describe exemplary workflows for the analysis of qPCR, qIA or dPCR experiments including the analysis of melting curve data. Our analysis relies entirely on R packages available from public repositories. Additionally, we provide information related to standardized and reproducible research.},
   author = {Stefan Rödiger and Michał Burdukiewicz and Konstantin Blagodatskikh and Michael Jahn and Peter Schierack},
   doi = {10.32614/RJ-2015-011},
   issn = {20734859},
   issue = {1},
   journal = {R Journal},
   pages = {127-150},
   publisher = {Technische Universitaet Wien},
   title = {R as an environment for reproducible analysis of DNA amplification experiments},
   volume = {7},
   year = {2015},
}
@article{Rodiger2017,
   abstract = {Motivation: Reproducibility, a cornerstone of research, requires defined data formats, which include the setup and output of experiments. The real-time PCR data markup language (RDML) is a recommended standard of the minimum information for publication of quantitative real-time PCR experiments guidelines. Despite the popularity of the RDML format for analysis of quantitative PCR data, handling of RDML files is not yet widely supported in all PCR curve analysis softwares. Results: This study describes the open-source RDML package for the statistical computing language R. RDML is compatible with RDML versions 1.2 and provides functionality to (i) import RDML data; (ii) extract sample information (e.g. targets and concentration); (iii) transform data to various formats of the R environment; (iv) generate human-readable run summaries; and (v) to create RDML files from user data. In addition, RDML offers a graphical user interface to read, edit and create RDML files.},
   author = {Stefan Rödiger and Michal Burdukiewicz and Andrej Nikolai Spiess and Konstantin Blagodatskikh},
   doi = {10.1093/BIOINFORMATICS/BTX528},
   issn = {1367-4803},
   issue = {24},
   journal = {Bioinformatics},
   month = {12},
   pages = {4012-4014},
   pmid = {28961912},
   publisher = {Oxford Academic},
   title = {Enabling reproducible real-time quantitative PCR research: the RDML package},
   volume = {33},
   url = {https://academic.oup.com/bioinformatics/article/33/24/4012/4095640},
   year = {2017},
}
@article{Pabinger2014,
   abstract = {Real-time quantitative polymerase-chain-reaction (qPCR) is a standard technique in most laboratoriesused for various applications in basic research. Analysis of qPCR data is a crucial part of the entire experiment,which has led to the development of a plethora of methods. The released tools either cover specificparts of the workflow or provide complete analysis solutions. Here, we surveyed 27 open-access software packages and tools for the analysis of qPCR data. The surveyincludes 8 Microsoft Windows, 5 web-based, 9 R-based and 5 tools from other platforms. Reviewedpackages and tools support the analysis of different qPCR applications, such as RNA quantification, DNAmethylation, genotyping, identification of copy number variations, and digital PCR. We report an overviewof the functionality, features and specific requirements of the individual software tools, such as dataexchange formats, availability of a graphical user interface, included procedures for graphical data presentation,and offered statistical methods. In addition, we provide an overview about quantificationstrategies, and report various applications of qPCR. Our comprehensive survey showed that most tools use their own file format and only a fraction ofthe currently existing tools support the standardized data exchange format RDML. To allow a morestreamlined and comparable analysis of qPCR data, more vendors and tools need to adapt the standardizedformat to encourage the exchange of data between instrument software, analysis tools, and researchers.},
   author = {Stephan Pabinger and Stefan Rödiger and Albert Kriegner and Klemens Vierlinger and Andreas Weinhäusel},
   doi = {10.1016/J.BDQ.2014.08.002},
   issn = {22147535},
   issue = {1},
   journal = {Biomolecular Detection and Quantification},
   keywords = {Data analysis,MIQE,RDML,Software,Tools,qPCR},
   pages = {23-33},
   publisher = {Elsevier GmbH},
   title = {A survey of tools for the analysis of quantitative PCR (qPCR) data},
   volume = {1},
   year = {2014},
}
@article{Krahenbuhl2019,
   abstract = {The Electronic Laboratory Information and Management Utensil for Molecular Diagnostics (ELIMU-MDx) is a user-friendly platform designed and built to accelerate the turnaround time of diagnostic qPCR assays. ELIMU-MDx is compliant with the MIQE guidelines and has extensive data-import capabilities for all major qPCR instruments by using the RDML data standard. This platform was designed as an open-source software tool and can be accessed through the web browser on all major operating systems. METHOD SUMMARY ELIMU-MDx is an open-source web-application developed using PHP to analyze, manage, validate and store user-provided qPCR data in a MySQL database.},
   author = {Silvan Krähenbühl and Fabian Studer and Etienne Guirou and Anna Deal and Philipp Mächler and Salome Hosch and Maximilian Mpina and Sarah Mswata and Claudia Daubenberger and Tobias Schindler},
   doi = {10.2144/BTN-2019-0064/ASSET/IMAGES/LARGE/FIGURE3.JPEG},
   issn = {19409818},
   issue = {1},
   journal = {BioTechniques},
   keywords = {Diagnostic,ELIMU-MDx,Infectious diseases,MIQE,QPCR,RDML},
   month = {10},
   pages = {22-27},
   pmid = {31588775},
   publisher = {Future Science Ltd},
   title = {ELIMU-MDx: A web-based, open-source platform for storage, management and analysis of diagnostic qPCR data},
   volume = {68},
   url = {https://www.future-science.com/doi/abs/10.2144/btn-2019-0064},
   year = {2019},
}
@article{Rancurel2019,
   abstract = {SATQPCR is a web tool providing statistical analysis of real-time quantitative PCR data including all MIQE rules (gene efficiency, selection of reference genes and normalization with them). Our application is a quick tool that provides to the biologist, graphs as well as statistical tables summarizing their results with the chosen methods (t-test or ANOVA with Tukey test). The application is available at http://satqpcr.sophia.inra.fr with a demo dataset. Source code can be found at https://framagit.org/. Supplementary information: Tutorials at http://satqpcr.sophia.inra.fr/cgi/help.cgi;},
   author = {Corinne Rancurel and T. van Tran and Céline Elie and Frédérique Hilliou},
   doi = {10.1016/J.MCP.2019.07.001},
   issn = {0890-8508},
   journal = {Molecular and Cellular Probes},
   keywords = {Data analysis,MIQE,Real-time quantitative PCR,Statistics,Transcriptomic,Web-application},
   month = {8},
   pages = {101418},
   pmid = {31283967},
   publisher = {Academic Press},
   title = {SATQPCR: Website for statistical analysis of real-time quantitative PCR data},
   volume = {46},
   year = {2019},
}
@article{Bustin2017,
   abstract = {Poorly executed and inadequately reported molecular measurement methods are amongst the causes underlying the lack of reproducibility of much biomedical research. Although several high impact factor journals have acknowledged their past failure to scrutinise adequately the technical soundness of manuscripts, there is a perplexing reluctance to implement basic corrective measures. The reverse transcription real-time quantitative PCR (RT-qPCR) is probably the most straightforward measurement technique available for RNA quantification and is widely used in research, diagnostic, forensic and biotechnology applications. Despite the impact of the minimum information for the publication of quantitative PCR experiments (MIQE) guidelines, which aim to improve the robustness and the transparency of reporting of RT-qPCR data, we demonstrate that elementary protocol errors, inappropriate data analysis and inadequate reporting continue to be rife and conclude that the majority of published RT-qPCR data are likely to represent technical noise.},
   author = {Stephen Bustin and Tania Nolan},
   doi = {10.1111/ECI.12801},
   issn = {1365-2362},
   issue = {10},
   journal = {European Journal of Clinical Investigation},
   keywords = {Gene expression,qPCR,quantification,reverse transcription},
   month = {10},
   pages = {756-774},
   pmid = {28796277},
   publisher = {John Wiley & Sons, Ltd},
   title = {Talking the talk, but not walking the walk: RT-qPCR as a paradigm for the lack of reproducibility in molecular research},
   volume = {47},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1111/eci.12801 https://onlinelibrary.wiley.com/doi/abs/10.1111/eci.12801 https://onlinelibrary.wiley.com/doi/10.1111/eci.12801},
   year = {2017},
}
@article{Courts2019,
   author = {Cornelius Courts and Michael W. Pfaffl and Eva Sauer and Walther Parson},
   doi = {10.1016/J.FSIGEN.2019.06.021},
   issn = {1872-4973},
   journal = {Forensic Science International: Genetics},
   month = {9},
   pages = {e21-e24},
   pmid = {31270013},
   publisher = {Elsevier},
   title = {Pleading for adherence to the MIQE-Guidelines when reporting quantitative PCR data in forensic genetic research},
   volume = {42},
   year = {2019},
}
@article{Zanardi2019,
   abstract = {Reverse transcription quantitative real-time polymerase chain reaction (RT-qPCR) is an accurate and fast method to measure gene expression. Reproducibility of the analyses is the main limitation of RT-qPCR experiments. Galaxy is an open, web-based, genomic workbench for a reproducible, transparent, and accessible science. Our aim was developing a new Galaxy tool for the analysis of RT-qPCR expression data. Our tool was developed using Galaxy workbench version 19.01 and functions implemented in several R packages. We developed PIPE-T, a new Galaxy tool implementing a workflow, which offers several options for parsing, filtering, normalizing, imputing, and analyzing RT-qPCR data. PIPE-T requires two input files and returns seven output files. We tested the ability of PIPE-T to analyze RT-qPCR data on two example datasets available in the gene expression omnibus repository. In both cases, our tool successfully completed execution returning expected results. PIPE-T can be easily installed from the Galaxy main tool shed or from Docker. Source code, step-by-step instructions, and example files are available on GitHub to assist new users to install, execute, and test PIPE-T. PIPE-T is a new tool suitable for the reproducible, transparent, and accessible analysis of RT-qPCR expression data.},
   author = {Nicolò Zanardi and Martina Morini and Marco Antonio Tangaro and Federico Zambelli and Maria Carla Bosco and Luigi Varesio and Alessandra Eva and Davide Cangelosi},
   doi = {10.1038/s41598-019-53155-9},
   issn = {2045-2322},
   issue = {1},
   journal = {Scientific Reports 2019 9:1},
   keywords = {Software,Statistical methods,Transcriptomics},
   month = {11},
   pages = {1-12},
   pmid = {31772190},
   publisher = {Nature Publishing Group},
   title = {PIPE-T: a new Galaxy tool for the analysis of RT-qPCR expression data},
   volume = {9},
   url = {https://www.nature.com/articles/s41598-019-53155-9},
   year = {2019},
}
@article{Ahmed2018,
   abstract = {Background. Real-time quantitative PCR (qPCR) is a broadly used technique in the biomedical research. Currently, few different analysis models are used to determine the quality of data and to quantify the mRNA level across the experimental conditions. Methods. We developed an R package to implement methods for quality assessment, analysis and testing qPCR data for statistical significance. Double Delta CT and standard curve models were implemented to quantify the relative expression of target genes from CT in standard qPCR control-group experiments. In addition, calculation of amplification efficiency and curves from serial dilution qPCR experiments are used to assess the quality of the data. Finally, two-group testing and linear models were used to test for significance of the difference in expression control groups and conditions of interest. Results. Using two datasets from qPCR experiments, we applied different quality assessment, analysis and statistical testing in the pcr package and compared the results to the original published articles. The final relative expression values from the different models, as well as the intermediary outputs, were checked against the expected results in the original papers and were found to be accurate and reliable. Conclusion. The pcr package provides an intuitive and unified interface for its main functions to allow biologist to perform all necessary steps of qPCR analysis and produce graphs in a uniform way.},
   author = {Mahmoud Ahmed and Deok Ryong Kim},
   doi = {10.7717/PEERJ.4473},
   issn = {2167-8359},
   issue = {3},
   journal = {PeerJ},
   keywords = {Deok Ryong Kim,MEDLINE,Mahmoud Ahmed,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC5858653,PubMed Abstract,doi:10.7717/peerj.4473,pmid:29576953},
   pmid = {29576953},
   publisher = {PeerJ},
   title = {pcr: an R package for quality assessment, analysis and testing of qPCR data},
   volume = {6},
   url = {https://pubmed.ncbi.nlm.nih.gov/29576953/},
   year = {2018},
}
@article{OlaecheaLazaro2021,
   abstract = {Background: Quantitative, reverse transcription PCR (qRT-PCR) is currently the gold-standard for SARS-CoV-2 detection and it is also used for detection of other virus. Manual data analysis of a small number of qRT-PCR plates per day is a relatively simple task, but automated, integrative strategies are needed if a laboratory is dealing with hundreds of plates per day, as is being the case in the COVID-19 pandemic. Results: Here we present shinyCurves, an online shiny-based, free software to analyze qRT-PCR amplification data from multi-plate and multi-platform formats. Our shiny application does not require any programming experience and is able to call samples Positive, Negative or Undetermined for viral infection according to a number of user-defined settings, apart from providing a complete set of melting and amplification curve plots for the visual inspection of results. Conclusions: shinyCurves is a flexible, integrative and user-friendly software that speeds-up the analysis of massive qRT-PCR data from different sources, with the possibility of automatically producing and evaluating melting and amplification curve plots.},
   author = {S. Olaechea-Lázaro and I. García-Santisteban and J. R. Pineda and I. Badiola and S. Alonso and Jose Ramon Bilbao and Nora Fernandez-Jimenez},
   doi = {10.1186/S12859-021-04392-1/FIGURES/1},
   issn = {14712105},
   issue = {1},
   journal = {BMC Bioinformatics},
   keywords = {COVID-19,Data analysis,Diagnosis,Medical informatics,Melting and amplification curves,Shiny application,Virology,qRT-PCR},
   month = {12},
   pages = {1-6},
   pmid = {34602053},
   publisher = {BioMed Central Ltd},
   title = {shinyCurves, a shiny web application to analyse multisource qPCR amplification data: a COVID-19 case study},
   volume = {22},
   url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04392-1},
   year = {2021},
}
@article{Ng2021,
   abstract = {Relative quantification is a popular analysis in gene expression studies using quantitative real-time PCR (qPCR). However, the calculation steps using the major algorithms for this analysis are rather complicated. In this study, we developed an easy-to-use spreadsheet-based method for relative quantification. The inputs from end-users are the efficiencies of both target and reference genes and the Cq values of those genes from cases and controls. This method performed normalization (with one or more reference genes), calculation of fold change of gene expression, and statistical analysis to analyze the difference between the groups in a step-by-step manner, which would allow the end-users to understand how the analysis arrived at the conclusion. Four previously published data sets with different experimental designs were used as examples. The calculated results were concordant with the results computed by the Relative Expression Software Tool (REST) 2009, a popular tool for relative quantification. Altogether, our method, which offers easy-to-understand calculation steps and does not require specialized instruments, software, or expertise to operate, would be a useful tool for students, educators, and scientists in the field of molecular biology.},
   author = {Hien Fuh Ng and Yun Fong Ngeow and Abdul Rahman and Malaysia Correspondence and Yun Fong Ngeow},
   doi = {10.1002/BMB.21596},
   issn = {1539-3429},
   journal = {Biochemistry and Molecular Biology Education},
   keywords = {gene expression studies,quantitative real time PCR,relative quantification,spreadsheet},
   publisher = {John Wiley & Sons, Ltd},
   title = {A simple spreadsheet-based method for relative quantification using quantitative real-time PCR},
   url = {https://onlinelibrary.wiley.com/doi/full/10.1002/bmb.21596 https://onlinelibrary.wiley.com/doi/abs/10.1002/bmb.21596 https://iubmb.onlinelibrary.wiley.com/doi/10.1002/bmb.21596},
   year = {2021},
}
@article{Maussion2021,
   abstract = {Quantifying changes in DNA and RNA levels is essential in numerous molecular biology protocols. Quantitative real time PCR (qPCR) techniques have evolved to become commonplace, however, data analysis includes many time-consuming and cumbersome steps, which can lead to mistakes and misinterpretation of data. To address these bottlenecks, we have developed an open-source Python software to automate processing of result spreadsheets from qPCR machines, employing calculations usually performed manually. Auto-qPCR is a tool that saves time when computing qPCR data, helping to ensure reproducibility of qPCR experiment analyses. Our web-based app (
                https://auto-q-pcr.com/
                
              ) is easy to use and does not require programming knowledge or software installation. Using Auto-qPCR, we provide examples of data treatment, display and statistical analyses for four different data processing modes within one program: (1) DNA quantification to identify genomic deletion or duplication events; (2) assessment of gene expression levels using an absolute model, and relative quantification (3) with or (4) without a reference sample. Our open access Auto-qPCR software saves the time of manual data analysis and provides a more systematic workflow, minimizing the risk of errors. Our program constitutes a new tool that can be incorporated into bioinformatic and molecular biology pipelines in clinical and research labs.},
   author = {Gilles Maussion and Rhalena A. Thomas and Iveta Demirova and Gracia Gu and Eddie Cai and Carol X.Q. Chen and Narges Abdian and Theodore J.P. Strauss and Sabah Kelaï and Angela Nauleau-Javaudin and Lenore K. Beitel and Nicolas Ramoz and Philip Gorwood and Thomas M. Durcan},
   doi = {10.1038/s41598-021-99727-6},
   isbn = {0123456789},
   issn = {2045-2322},
   issue = {1},
   journal = {Scientific Reports 2021 11:1},
   keywords = {Bioinformatics,Data processing,PCR,Pluripotent stem cells,Reverse transcription polymerase chain reaction,Software,Stem,Transcriptomics,based techniques,cell differentiation},
   month = {10},
   pages = {1-14},
   pmid = {34716395},
   publisher = {Nature Publishing Group},
   title = {Auto-qPCR; a python-based web app for automated and reproducible analysis of qPCR data},
   volume = {11},
   url = {https://www.nature.com/articles/s41598-021-99727-6},
   year = {2021},
}
@article{Bustin2009,
   abstract = {BACKGROUND: Currently, a lack of consensus exists on how best to perform and interpret quantitative real- time PCR (qPCR) experiments. The problem is exac- erbated by a lack of sufficient experimental detail in many publications, which impedes a reader's ability to evaluate critically the quality of the results presented or to repeat the experiments. CONTENT: The Minimum Information for Publication of Quantitative Real-Time PCR Experiments (MIQE) guidelines target the reliability of results to help ensure the integrity of the scientific literature, promote con- sistency between laboratories, and increase experimen- tal transparency. MIQE is a set of guidelines that de- scribe the minimum information necessary for evaluating qPCR experiments. Included is a checklist to accompany the initial submission of a manuscript to the publisher. By providing all relevant experimental conditions and assay characteristics, reviewers can as- sess the validity of the protocols used. Full disclosure of all reagents, sequences, and analysis methods is neces- sary to enable other investigators to reproduce results. MIQE details should be published either in abbreviated form or as an online supplement. SUMMARY: Following these guidelines will encourage better experimental practice, allowing more reliable and unequivocal interpretation of qPCR results. © 2009 American Association for Clinical Chemistry.},
   author = {Stephen A. Bustin and Vladimir Benes and Jeremy A. Garson and Jan Hellemans and Jim Huggett and Mikael Kubista and Reinhold Mueller and Tania Nolan and Michael W. Pfaffl and Gregory L. Shipley and Jo Vandesompele and Carl T. Wittwer},
   doi = {10.1373/CLINCHEM.2008.112797},
   issn = {1530-8561},
   issue = {4},
   journal = {Clinical chemistry},
   keywords = {Carl T Wittwer,Guideline,Humans,MEDLINE,Molecular Diagnostic Techniques,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,Nucleic Acids / chemistry,Nucleic Acids / genetics,Polymerase Chain Reaction / methods*,Polymerase Chain Reaction / standards*,PubMed Abstract,Publishing / standards*,Research Support,Reverse Transcription / genetics,Stephen A Bustin,Terminology as Topic,Time Factors,Vladimir Benes,doi:10.1373/clinchem.2008.112797,pmid:19246619},
   month = {4},
   pages = {611-622},
   pmid = {19246619},
   publisher = {Clin Chem},
   title = {The MIQE guidelines: minimum information for publication of quantitative real-time PCR experiments},
   volume = {55},
   url = {https://pubmed.ncbi.nlm.nih.gov/19246619/},
   year = {2009},
}
@article{Geiger2017,
   abstract = {This report is a high-level summary analysis of the 2017 GitHub Open Source Survey dataset, presenting frequency counts, proportions, and frequency or proportion bar plots for every question asked in the survey.},
   author = {R. Stuart Geiger},
   doi = {10.17605/OSF.IO/ENRQ5},
   month = {6},
   title = {Summary Analysis of the 2017 GitHub Open Source Survey},
   url = {http://arxiv.org/abs/1706.02777 http://dx.doi.org/10.17605/OSF.IO/ENRQ5},
   year = {2017},
}
@article{Dvinge2009,
   abstract = {Motivation: Quantitative real-time polymerase chain reaction (qPCR) is routinely used for RNA expression profiling, validation of microarray hybridization data and clinical diagnostic assays. Although numerous statistical tools are available in the public domain for the analysis of microarray experiments, this is not the case for qPCR. Proprietary software is typically provided by instrument manufacturers, but these solutions are not amenable to the tandem analysis of multiple assays. This is problematic when an experiment involves more than a simple comparison between a control and treatment sample, or when many qPCR datasets are to be analyzed in a high-throughput facility.Results: We have developed HTqPCR, a package for the R statistical computing environment, to enable the processing and analysis of qPCR data across multiple conditions and replicates. © The Author(s) 2009. Published by Oxford University Press.},
   author = {Heidi Dvinge and Paul Bertone},
   doi = {10.1093/BIOINFORMATICS/BTP578},
   issn = {1367-4811},
   issue = {24},
   journal = {Bioinformatics (Oxford, England)},
   keywords = {Computational Biology / methods*,Databases,Gene Expression Profiling,Genetic,Heidi Dvinge,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,PMC2788924,Paul Bertone,Polymerase Chain Reaction / methods*,PubMed Abstract,Research Support,Software*,doi:10.1093/bioinformatics/btp578,pmid:19808880},
   month = {10},
   pages = {3325-3326},
   pmid = {19808880},
   publisher = {Bioinformatics},
   title = {HTqPCR: high-throughput analysis and visualization of quantitative real-time PCR data in R},
   volume = {25},
   url = {https://pubmed.ncbi.nlm.nih.gov/19808880/},
   year = {2009},
}
@article{Ritz2008,
   abstract = {Summary: The qpcR library is an add-on to the free R statistical environment performing sigmoidal model selection in real-time quantitative polymerase chain reaction (PCR) data analysis. Additionally, the package implements the most commonly used algorithms for real-time PCR data analysis and is capable of extensive statistical comparison for the selection and evaluation of the different models based on several measures of goodness of fit. © The Author 2008. Published by Oxford University Press. All rights reserved.},
   author = {Christian Ritz and Andrej Nikolai Spiess},
   doi = {10.1093/BIOINFORMATICS/BTN227},
   issn = {1367-4803},
   issue = {13},
   journal = {Bioinformatics},
   month = {7},
   pages = {1549-1551},
   pmid = {18482995},
   publisher = {Oxford Academic},
   title = {qpcR: an R package for sigmoidal model selection in quantitative real-time polymerase chain reaction analysis},
   volume = {24},
   url = {https://academic.oup.com/bioinformatics/article/24/13/1549/238435},
   year = {2008},
}
@article{Stahlberg2004,
   abstract = {Background: In most measurements of gene expression, mRNA is first reverse-transcribed into cDNA. We studied the reverse transcription reaction and its consequences for quantitative measurements of gene expression. Methods: We used SYBR green I-based quantitative real-time PCR (QPCR) to measure the properties of reverse transcription reaction for the β-tubulin, glyceraldehyde-3-phosphate dehydrogenase, Glut2, CaV1D, and insulin II genes, using random hexamers, oligo(dT), and gene-specific reverse transcription primers. Results: Experimental variation in reverse transcription-QPCR (RT-QPCR) was mainly attributable to the reverse transcription step. Reverse transcription efficiency depended on priming strategy, and the dependence was different for the five genes studied. Reverse transcription yields also depended on total RNA concentration. Conclusions: RT-QPCR gene expression measurements are comparable only when the same priming strategy and reaction conditions are used in all experiments and the samples contain the same total amount of RNA. Experimental accuracy is improved by running samples in (at least) duplicate starting with the reverse transcription reaction. © 2004 American Association for Clinical Chemistry.},
   author = {Anders Ståhlberg and Joakim Håkansson and Xiaojie Xian and Henrik Semb and Mikael Kubista},
   doi = {10.1373/CLINCHEM.2003.026161},
   issn = {0009-9147},
   issue = {3},
   journal = {Clinical Chemistry},
   month = {3},
   pages = {509-515},
   pmid = {14726469},
   publisher = {Oxford Academic},
   title = {Properties of the Reverse Transcription Reaction in mRNA Quantification},
   volume = {50},
   url = {https://academic.oup.com/clinchem/article/50/3/509/5639817},
   year = {2004},
}

@Manual{,
    title = {tm: Text Mining Package},
    author = {Ingo Feinerer and Kurt Hornik},
    year = {2020},
    note = {R package version 0.7-8},
    url = {https://CRAN.R-project.org/package=tm},
}

@Article{,
    title = {Text Mining Infrastructure in R},
    author = {Ingo Feinerer and Kurt Hornik and David Meyer},
    year = {2008},
    journal = {Journal of Statistical Software},
    volume = {25},
    number = {5},
    pages = {1--54},
    url = {https://www.jstatsoft.org/v25/i05/},
    month = {March},
}

@Manual{,
    title = {wordcloud: Word Clouds},
    author = {Ian Fellows},
    year = {2018},
    note = {R package version 2.6},
    url = {https://CRAN.R-project.org/package=wordcloud},
  }
  
@Manual{,
    title = {pluralize: Pluralize and 'Singularize' Any (English) Word},
    author = {Bob Rudis and Blake Embrey},
    year = {2020},
    note = {R package version 0.2.0},
    url = {https://CRAN.R-project.org/package=pluralize},
}
@article{AbdelNour2020,
   abstract = {The Polymerase Chain Reaction (PCR) is the most valuable tool that marked the history of molecular biology since the discovery of the DNA structure by Watson and Crick. Its impact on human health over the last 30 years had lead researchers in the field to put strict rules referred to as the Minimum Information for publication of Quantitative real-time PCR Experiments (MIQE) guidelines since 2009. The aim of the current analysis is to shed light on the practice of applying and citing the original MIQE in the published articles over the last decade. We showed that qPCR is a global technique, but the usage of the MIQE is still lagging in the emerging economies around the globe. We have shown that researchers following the MIQE have better chances of publishing highly cited papers. The MIQE represent the laws for this technique: they enslave us, molecular biologists, into a strict path with financial burdens, but they free us from the “human-errors” a machine would impose on us. As science seeks perfection especially when dealing with human problems, the MIQE, as assessed by the publications' quality over the years, did indeed achieve this step forward towards making the PCR an infallible tool.},
   author = {Afif M. Abdel Nour and Georges Nemer and Athar Khalil},
   doi = {10.1016/J.GENREP.2020.100630},
   issn = {2452-0144},
   journal = {Gene Reports},
   keywords = {CiteSCore,MIQE,qPCR},
   month = {6},
   pages = {100630},
   publisher = {Elsevier},
   title = {The MIQE Guidelines' tenth anniversary: The good and bad students},
   volume = {19},
   year = {2020},
}
@article{Burns2005,
   abstract = {Background: As real-time quantitative PCR (RT-QPCR) is increasingly being relied upon for the enforcement of legislation and regulations dependent upon the trace detection of DNA, focus has increased on the quality issues related to the technique. Recent work has focused on the identification of factors that contribute towards significant measurement uncertainty in the real-time quantitative PCR technique, through investigation of the experimental design and operating procedure. However, measurement uncertainty contributions made during the data analysis procedure have not been studied in detail. This paper presents two additional approaches for standardising data analysis through the novel application of statistical methods to RT-QPCR, in order to minimise potential uncertainty in results. Results: Experimental data was generated in order to develop the two aspects of data handling and analysis that can contribute towards measurement uncertainty in results. This paper describes preliminary aspects in standardising data through the application of statistical techniques to the area of RT-QPCR. The first aspect concerns the statistical identification and subsequent handling of outlying values arising from RT-QPCR, and discusses the implementation of ISO guidelines in relation to acceptance or rejection of outlying values. The second aspect relates to the development of an objective statistical test for the comparison of calibration curves. Conclusions: The preliminary statistical tests for outlying values and comparisons between calibration curves can be applied using basic functions found in standard spreadsheet software. These two aspects emphasise that the comparability of results arising from RT-QPCR needs further refinement and development at the data-handling phase. The implementation of standardised approaches to data analysis should further help minimise variation due to subjective judgements. The aspects described in this paper will help contribute towards the development of a set of best practice guidelines regarding standardising handling and interpretation of data arising from RT-QPCR experiments. © 2005 Burns et al., licensee BioMed Central Ltd.},
   author = {Malcolm J. Burns and Gavin J. Nixon and Carole A. Foy and Neil Harris},
   doi = {10.1186/1472-6750-5-31},
   issn = {1472-6750},
   journal = {BMC biotechnology},
   keywords = {Biotechnology / methods*,Calibration,Clinical Laboratory Techniques,DNA Primers,Data Interpretation,Evaluation Studies as Topic,Gavin J Nixon,Glyceraldehyde-3-Phosphate Dehydrogenase (Phosphorylating) / genetics,Humans,MEDLINE,Malcolm J Burns,Models,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Neil Harris,Non-U.S. Gov't,PMC1326201,PubMed Abstract,Reference Standards,Regression Analysis,Reproducibility of Results,Research Design,Research Support,Reverse Transcriptase Polymerase Chain Reaction / methods*,Sensitivity and Specificity,Software,Statistical,Theoretical,doi:10.1186/1472-6750-5-31,pmid:16336641},
   month = {12},
   pmid = {16336641},
   publisher = {BMC Biotechnol},
   title = {Standardisation of data from real-time quantitative PCR methods - evaluation of outliers and comparison of calibration curves},
   volume = {5},
   url = {https://pubmed.ncbi.nlm.nih.gov/16336641/},
   year = {2005},
}
@article{Lefever2009,
   abstract = {The XML-based Real-Time PCR Data Markup Language (RDML) has been developed by the RDML consortium (http://www.rdml.org) to enable straightforward exchange of qPCR data and related information between qPCR instruments and third party data analysis software, between colleagues and collaborators and between experimenters and journals or public repositories. We here also propose data related guidelines as a subset of the Minimum Information for Publication of Quantitative Real-Time PCR Experiments (MIQE) to guarantee inclusion of key data information when reporting experimental results.},
   author = {Steve Lefever and Jan Hellemans and Filip Pattyn and Daniel R. Przybylski and Chris Taylor and René Geurts and Andreas Untergasser and Jo Vandesompele},
   doi = {10.1093/NAR/GKP056},
   issn = {1362-4962},
   issue = {7},
   journal = {Nucleic acids research},
   keywords = {CollabAuthor(name='RDML consortium',Guidelines as Topic*,Internet,Jan Hellemans,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-U.S. Gov't,PMC2673419,Polymerase Chain Reaction / standards*,PubMed Abstract,Research Support,Software*,Steve Lefever,Terminology as Topic*,affs=[]),doi:10.1093/nar/gkp056,investigators=[],pmid:19223324},
   pages = {2065-2069},
   pmid = {19223324},
   publisher = {Nucleic Acids Res},
   title = {RDML: structured language and reporting guidelines for real-time quantitative PCR data},
   volume = {37},
   url = {https://pubmed.ncbi.nlm.nih.gov/19223324/},
   year = {2009},
}
@article{Ruijter2015,
   abstract = {Background: The universal qPCR data exchange file format RDML is today well accepted by the scientific community, part of the MIQE guidelines and implemented in many qPCR instruments. With the increased use of RDML new challenges emerge. The flexibility of the RDML format resulted in some implementations that did not meet the expectations of the consortium in the level of support or the use of elements. Results: In the current RDML version 1.2 the description of the elements was sharpened. The open source editor RDML-Ninja was released (http://sourceforge.net/projects/qpcr-ninja/). RDML-Ninja allows to visualize, edit and validate RDML files and thus clarifies the use of RDML elements. Furthermore RDML-Ninja serves as reference implementation for RDML and enables migration between RDML versions independent of the instrument software. The database RDMLdb will serve as an online repository for RDML files and facilitate the exchange of RDML data (http://www.rdmldb.org). Authors can upload their RDML files and reference them in publications by the unique identifier provided by RDMLdb. The MIQE guidelines propose a rich set of information required to document each qPCR run. RDML provides the vehicle to store and maintain this information and current development aims at further integration of MIQE requirements into the RDML format. Conclusions: The editor RDML-Ninja and the database RDMLdb enable scientists to evaluate and exchange qPCR data in the instrument-independent RDML format. We are confident that this infrastructure will build the foundation for standardized qPCR data exchange among scientists, research groups, and during publication.},
   author = {Jan M. Ruijter and Steve Lefever and Jasper Anckaert and Jan Hellemans and Michael W. Pfaffl and Vladimir Benes and Stephen A. Bustin and Jo Vandesompele and Andreas Untergasser},
   doi = {10.1186/S12859-015-0637-6},
   issn = {1471-2105},
   issue = {1},
   journal = {BMC bioinformatics},
   keywords = {CollabAuthor(name='RDML consortium',Computer Communication Networks / standards*,Databases,Factual*,Humans,Jan M Ruijter,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC4474546,Polymerase Chain Reaction / methods*,PubMed Abstract,Software*,Steve Lefever,affs=[]),doi:10.1186/s12859-015-0637-6,investigators=[],pmid:26087842},
   month = {6},
   pmid = {26087842},
   publisher = {BMC Bioinformatics},
   title = {RDML-Ninja and RDMLdb for standardized exchange of qPCR data},
   volume = {16},
   url = {https://pubmed.ncbi.nlm.nih.gov/26087842/},
   year = {2015},
}
@software{Kleinschmidt2022,
author = {Kleinschmidt, N.},
year = {2022},
title = {qpcr - a python module for easy and versatile qPCR data analysis for small-scale datasets and high-throughput},
version = {3.1.5},
url =  {https://github.com/NoahHenrikKleinschmidt/qpcr.git},
}
@article{Kubista2006,
title = {The real-time polymerase chain reaction},
journal = {Molecular Aspects of Medicine},
volume = {27},
number = {2},
pages = {95-125},
year = {2006},
note = {Real-time Polymerase Chain Reaction},
issn = {0098-2997},
doi = {https://doi.org/10.1016/j.mam.2005.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0098299705000907},
author = {Mikael Kubista and José Manuel Andrade and Martin Bengtsson and Amin Forootan and Jiri Jonák and Kristina Lind and Radek Sindelka and Robert Sjöback and Björn Sjögreen and Linda Strömbom and Anders Ståhlberg and Neven Zoric},
keywords = {Real-time PCR, Gene expression profiling, GenEx, Principal component analysis, PCA, Multidimensional expression profiling},
abstract = {The scientific, medical, and diagnostic communities have been presented the most powerful tool for quantitative nucleic acids analysis: real-time PCR [Bustin, S.A., 2004. A–Z of Quantitative PCR. IUL Press, San Diego, CA]. This new technique is a refinement of the original Polymerase Chain Reaction (PCR) developed by Kary Mullis and coworkers in the mid 80:ies [Saiki, R.K., et al., 1985. Enzymatic amplification of β-globin genomic sequences and restriction site analysis for diagnosis of sickle cell anemia, Science 230, 1350], for which Kary Mullis was awarded the 1993 year’s Nobel prize in Chemistry. By PCR essentially any nucleic acid sequence present in a complex sample can be amplified in a cyclic process to generate a large number of identical copies that can readily be analyzed. This made it possible, for example, to manipulate DNA for cloning purposes, genetic engineering, and sequencing. But as an analytical technique the original PCR method had some serious limitations. By first amplifying the DNA sequence and then analyzing the product, quantification was exceedingly difficult since the PCR gave rise to essentially the same amount of product independently of the initial amount of DNA template molecules that were present. This limitation was resolved in 1992 by the development of real-time PCR by Higuchi et al. [Higuchi, R., Dollinger, G., Walsh, P.S., Griffith, R., 1992. Simultaneous amplification and detection of specific DNA-sequences. Bio-Technology 10(4), 413–417]. In real-time PCR the amount of product formed is monitored during the course of the reaction by monitoring the fluorescence of dyes or probes introduced into the reaction that is proportional to the amount of product formed, and the number of amplification cycles required to obtain a particular amount of DNA molecules is registered. Assuming a certain amplification efficiency, which typically is close to a doubling of the number of molecules per amplification cycle, it is possible to calculate the number of DNA molecules of the amplified sequence that were initially present in the sample. With the highly efficient detection chemistries, sensitive instrumentation, and optimized assays that are available today the number of DNA molecules of a particular sequence in a complex sample can be determined with unprecedented accuracy and sensitivity sufficient to detect a single molecule. Typical uses of real-time PCR include pathogen detection, gene expression analysis, single nucleotide polymorphism (SNP) analysis, analysis of chromosome aberrations, and most recently also protein detection by real-time immuno PCR.}
}
@article{Bustin2002,
   abstract = {The fluorescence-based real-time reverse transcription PCR (RT-PCR) is widely used for the quantification of steady-state mRNA levels and is a critical tool for basic research, molecular medicine and biotechnology. Assays are easy to perform, capable of high throughput, and can combine high sensitivity with reliable specificity. The technology is evolving rapidly with the introduction of new enzymes, chemistries and instrumentation. However, while real-time RT-PCR addresses many of the difficulties inherent in conventional RT-PCR, it has become increasingly clear that it engenders new problems that require urgent attention. Therefore, in addition to providing a snapshot of the state-of-the-art in real-time RT-PCR, this review has an additional aim: it will describe and discuss critically some of the problems associated with interpreting results that are numerical and lend themselves to statistical analysis, yet whose accuracy is significantly affected by reagent and operator variability.},
   author = {S. A. Bustin},
   doi = {10.1677/JME.0.0290023},
   issn = {1479-6813},
   issue = {1},
   journal = {Journal of Molecular Endocrinology},
   month = {8},
   pages = {23-39},
   pmid = {12200227},
   publisher = {BioScientifica},
   title = {Quantification of mRNA using real-time reverse transcription PCR (RT-PCR): trends and problems},
   volume = {29},
   url = {https://jme.bioscientifica.com/view/journals/jme/29/1/23.xml},
   year = {2002},
}
@article{Bustin2013,
   author = {Stephen A. Bustin},
   doi = {10.5772/52844},
   isbn = {978-953-51-1021-7},
   journal = {Recent Advances in Autism Spectrum Disorders - Volume I},
   month = {3},
   publisher = {IntechOpen},
   title = {Why There Is no Link Between Measles Virus and Autism},
   url = {https://www.intechopen.com/chapters/41291},
   year = {2013},
}
@article{Garson2009,
   author = {Jeremy A. Garson and Jim F. Huggett and Stephen A. Bustin and Michael W. Pfaffl and Vladimir Benes and Jo Vandesompele and Gregory L. Shipley},
   doi = {10.1089/AID.2008.0270},
   issn = {08892229},
   issue = {3},
   journal = {https://home.liebertpub.com/aid},
   month = {3},
   pages = {377-378},
   pmid = {19292592},
   publisher = { Mary Ann Liebert, Inc.  140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA  },
   title = {Unreliable Real-Time PCR Analysis of Human Endogenous Retrovirus-W (HERV-W) RNA Expression and DNA Copy Number in Multiple Sclerosis},
   volume = {25},
   url = {https://www.liebertpub.com/doi/abs/10.1089/aid.2008.0270},
   year = {2009},
}