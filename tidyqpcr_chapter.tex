%
% Sample SBC book chapter
%
% This is a public-domain file.
%
% Charset: ISO8859-1 (latin-1) áéíóúç
%
\documentclass{SBCbookchapter}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazil,english]{babel}
\usepackage{graphicx}
\usepackage{array}

\author{Sam Haynes}
\title{User-Friendly, Open Source Research Software}

\begin{document}
\maketitle

\begin{abstract}
This chapter introduces open-source software practices, community development and the pedagogy of software documentation to promote the development of long-term, high impact research software. Focusing on the issue of the standardisation of qPCR analysis, the chapter contrasts the R package tidyqpcr, developed by the author, to other current software available. This use case highlights how quality software supported by comprehensive documentation can improve the quality of the entire experimental assay.


\end{abstract}

\section{Research Software}

The development of high-throughput, multi-omic experiments across the biological sciences has led to an unprecedented demand on software for research. In the late 90's less than 20\% of research papers mentioned the use of software in their research. In 2021, over 70\% of publications stored on pubmed cited the use of software. Furthermore, articles accepted in the highest impact journals cited more pieces of software on average. (sentence emphasising use of statistical software). Software developed specifically to answer research questions has rapidly become a cornerstone of the modern empirical method. \cite{Schindler2022}

Despite its importance, research software development remains a nascent subdomain of software development. The academic position of research software engineer was only coined in the late 2000's \cite{Prause2010} with the creation of the society of software engineers being founded in 2010. Exemplary research groups and institutions exist with the primary aim of developing software to answer, or facilitate others in answering, research questions (SSI etc). However, it still remains a domain primarily occupied by self-taught programmers as an aside from their primary domain of expertise.  The software itself is regularly developed with specific short term goals during the latter stages of scientific inquiry to provide statistical summaries and deduce significant discoveries. Therefore, practitioners are rarely trained in the standard development practices that are used across all other professional software development contexts and they often consider the analysis steps a final hurdle before completing a project/publication.

This leads to common patterns that define research software. Research software is typically written in high level scripting languages (R/Python), often to produce statistical summaries of complex datasets as a means to gather evidence to support a conclusion in the latter stages of scientific inquiry (typically. during the steps of writing a publication for submission).

\subsection{Software development practices}

As the practice of software development became a profession, practitioners have created frameworks to facilitate efficient and rigorous software development.
The first software development framework, and the one still most used in the sciences, is called Waterfall.
Waterfall introduces a structure to the coding practice by outlining a series of stages that are completed linearly.
It starts with a detailed specification before moving to development and finally implementation.
Alternative practices generally deviate from the waterfall ethos of leaving implementation to the very last stage.
Instead they prioritise creating a minimal viable product as soon as possible and testing its functionality.
Others even enable the specification to be altered and redefined as the project develops.
The flexibility to redefine the project as it is being developed is why the alternative practices are called agile. 

Two common agile practices in industry are scrum and extreme programming.
Scrum is the modern archetypal agile method.
Instead of fixing the development schedule as each stage much be completed sequentially the project is broken into sprints each iteratively adding some functionality which is reviewed tested and implemented before moving onto the next.
Constantly reviewing and testing the code enables programmers to catch bugs early (ensuring complex dependency issues don't arise) receive feedback on whether initial functionality is actually useful and achievable.
As its name suggests extreme programming pushes the principles of agile programming to the extreme
Updates to the software are expected on a weekly basis with additions tested and implemented. 
In addition programmers are expected to conduct paired programming. 
Two programmes work together at all time with only one coding while the other verbally dictates what should be added. 
Although this reduces the number of lines of code written per developer, with two coders constantly reviewing each other code the number of programming bugs introduced is reduced which is assumed to negate any reduction in productivity.

Does research software differ from general software? (in terms of aims, requirements and expectations)

Software development methodologies from Waterfall to Extreme focus entirely on efficient and accurate frameworks for writing code. 
Although these methodologies differ in the timing and extent of feedback from primary stakeholders none explicitly state time for writing documentation. 
Software is only as effective as its user.
Coding practices are highly developed. Efficient coding practices, interactions between teams and automatic integration and testing of work. In practice, software is limited by the users understanding of its implementation.
Documentation practices are less developed.

% https://scrumguides.org/docs/scrumguide/v2020/2020-Scrum-Guide-US.pdf#zoom=100

% https://hygger.io/blog/extreme-programming-waterfall-agile-kanban-scrum-lean/#extreme-programming-vs-waterfall

% https://ieeexplore.ieee.org/abstract/document/1231158?casa_token=6sbQr1xrIy4AAAAA:0GmEQG8RPMLhd5563BpXhyfQqSPd7Tw88hrhpX2uo4bu3LgewtZNrNKTYggK0JMFJpBvsFMUS2c

% https://peerj.com/articles/cs-835/#results--analysis-of-software-mentions

\subsubsection{Computation and the environment}

A fast growing aspect of software development practices is the contribution of compute to the emission of CO2.
It is estimated that the IT sector will be responsible for 20\% of the world's CO2 production by 2030. 
Incorporating carbon footprint estimates into coding specifications and benchmarking published code may be the norm in the future
Balancing efficiencies and speed for new software updates may need to be encouraged.
Depending on highly inefficient older code may be detrimental to the carbon footprint but the latest version my prioritise speed over memory/CPU usage and actually increase emissions.
In addition, programming philosophies may need to be developed. 
Time tends to be the limiting factor in many analyses so swapping to faster code is nearly always the priority. 
However, is there a limit to how much we prioritise speed? 
If each run emits more CO2 and we conduct far more runs than was initially thought necessary will that always be seen as the ideal.

Two common sequence aligners hi-sat 2 and star are noted for their significant difference in memory consumption.
Previously research has noted that memory usage is overtaking CPU as the leading energy consumer in severs.
Over allocating memory to a task can significantly increase energy usage with 90\% of energy spent on maintaining memory in the background regardless of load.
Interestingly, the consequences of agile development practises with weekly or even daily testing could contribute to significant increases CO2 emissions.
Tools such of GitHub actions do enable quick detection of bugs but does the significant increase in compute worth the cost?

% https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009324

% https://dl.acm.org/doi/pdf/10.1145/3076113.3076117?download=true

\subsection{Open source software development}

The growing expectation for a science that has its results open to all to read, data freely available to use and decisions that are completely transparent leads to an natural proclivity towards the already developed world of open software. The software development practices described above can be followed with a closed development process or an open one. It is fairly prolific across research to leave any code used in a research paper to be 'available upon request'. However, the encouragement of open research software practices offers many of the advantages currently seen in industry. The success of the Google supported Machine Learning framework TensorFlow showcases the enhanced re-usability, collaboration and accessibility enabled by adompting an Open Source framework. Specific to the academic setting,  it have been shown suggest that publications that follow open science practices tend to have increased citations. A modern parable for open source software development comes for the epidemiological modelling of the spread of COVID-19 by Prof Neil Ferguson at Imperial Collage London. Crucial to the justification of national lockdowns to kerb the spread of COVID-19, the model was actual developed 13 years prior using undocumented, closed source C++ code. After six weeks of intense revisions and refactoring, with direct help from Microsoft and GitHub software architects,  is now, arguably, the poster child for open source research software.

% https://github.com/mrc-ide/covid-sim

% https://www.nature.com/articles/nature10836?em_x=22

% https://elifesciences.org/articles/16800

\subsection{GUIs or CLIs}

\begin{itemize}
    \item Since the macintosh there has been a continued shift between GUI and CLI statistical software taking the lead \cite{ValeroMora2012}. 
    \item GUIs tend to be associated a dramatically lower learning curve but larger development and maintenance costs.
    \item Better user experience in a quality GUI than CLI \cite{Staggers2000}, however do GUI developers prioritise ease of use over quality of analysis by ignoring key steps (such as quality control).
    \item GUIs are also limited by the number of ways a user can interact with the screen (interactive operations), drop down boxes quickly become unwieldy if they list every possible action to take on an object. 
    \item CLIs are often quicker to develop (so can include the latest statistical methods), easier to scale to larger work loads, better reproducibility and cheaper to maintain.
    \item Without a unified framework to conduct statistical analysis (how you define models and manipulate data) there cannot be an intuitive structure to create and expand a GUI. \cite{Unwin2012}

\end{itemize}

\section{Software documentation}

\subsection{Why is research software documentation important}

Software is intended to be a tool used to complete tasks efficiently.
It alleviates some of the mundane and repetitive tasks to enable users to focus on the complex and abstract. 
However, the intent is not always matched by the execution as users may become frustrated with design idiosyncrasies or suboptimal implementations.
In fact, researchers often dread the steps in their experimental assays that require the use of software or the analysis stages of their studies.
An opinion quickly becoming untenable as the trend trend towards data rich, high throughput studies has led even the most rudimentary analyses to depend on software.
Indeed, as statistical methodologies are still being developed to explore the vast data landscape the practice of reanalysing old experiments is only going to be more common. 
Although the demands for better computational literacy in undergrad and postgrad education is partially addressing these issues only by demanding software meet users halfway by emphasising transparency and accessibility will the problem full resolved.

In order to support reproducibility and the user experience, research software needs to be rewarded on its usability and documentation as well as its functionality.
In order assess the quality of software documentation it is useful to consider the types of documentation broadly separated by its intended audience and content. 
Previous studies have recognised three categories of documentation: documentation of decisions, why did you chose this problem to solve and why did you chose this particular method to solve it; documentation of product, what is contained within this software implementation and how do users interact with it; and documentation of technicalities, how developers created the software and how maintainers can contribute to it. 
Any software intended to be shared contains some product documentation, but few research software projects outline technical details and fewer still mention any decisions made in development \cite{Geiger2018}.

Funding bodies and journals have acknowledged across the board that research data needs to be FAIR (Findable, Accessible, Interoperable and Reusable). \textbf{evidence on uptake of fair practices} However, few guidelines have been implemented on the software packages and analysis pipelines that create/enrich/analysis these dataset to generate conclusions. Are the same FAIR condition relevant to research software? What does software that is FAIR mean? % https://www.rd-alliance.org/system/files/FAIR4RS_Principles_v0.3_RDA-RFC.pdf.

% https://www.researchsquare.com/article/rs-1239393/v1

Open source documentation as well as the software itself helps combat unconscious knowledge (knowledge that the initial developer had but forgets that not everyone knows!) Unconcious knowledge can break early stage tasks especially when the primary developer leaves and there is too much of a steep learning curve for anyone to take over.  

% https://www.software.ac.uk/resources/publications/better-software-better-research

\subsection{Why is research software documentation poor?}

The existence of multiple categories of documentation leads to a diverse set of reasons why users may find a software’s documentation insufficient.
To quantitatively detect patterns in poor software documentation \cite{Aghajani2019} scowered mailing lists, github issues and stack overflow for issues posted by users pertaining problems in documentaion. 
Common issues were based on factually incorrect statements in the documentation, sections of code/functions without any documentation at all or documentation becoming out of date with the latest package versions.
Other issues discuss the ease at which API documentation could be found and searched at all, exactly what terms meant in specific contexts and not having quality translations of documentation in other languages.
As expected, a complete lack of documentation is the most common issue but on the other extreme is dense, unintelligible documentation that is difficult to maintain and search.

Every developer knows the importance of documentation, but new software continues to be created with poor documentation.
In a 2017 GitHub survey of OSS contributors, 93\% reported that “incomplete or outdated documentation is a pervasive problem” but “60\% of contributors say they rarely or never contribute to documentation” \cite{Geiger2017}.
Software documentation typically is the least credited part of software development with little time or funds allocated to its development.
Documentation writers are first to go in times of economic difficulty. \cite{Forward2002}
However, paradoxically writing software documentation requires the most diverse set of skills and experiences to enable people from different backgrounds and knowledge to jump right in at an appropriate level \cite{Geiger2018}.
Documentation writers also envelope a larger populous than those with technical programming skills as the software developers themselves can be limited in their ability to write accessible documentation by unconscious knowledge.

The skills and interests of software developers may not naturally align with the demands of writing documentation, but perhaps the issues actually lies in the lack of community between developer, documenters and users.
Rich Bowen, Community Manager for the CentOS project,  says "There’s common wisdom in the open source world: Everybody knows that the documentation is awful, that nobody wants to write it, and that this is just the way things are. But the truth is that there are lots of people who want to write the docs. We just make it too hard for them to participate. So they write articles on Stack Overflow, on their blogs, and third-party forums. Although this can be good, it’s also a great way for worst-practice solutions to bloom and gain momentum. Embracing these people and making them part of the official documentation effort for your project has many advantages." https://labs.quansight.org/blog/2020/03/documentation-as-a-way- to-build-community/

\subsection{What are the benefits of good research software documentation?}
\begin{itemize}
    \item Documentation acts as "a resource for learning and a second role: as an advertisement for the software project" and the current health of a project.\cite{Geiger2018}
    \item Documentation supports the formation of a developer and maintainer community. Shifting knowledge acquisition away from poorly moderated forums such as biostars or stackoverflow. Extends accessibility of code by highlighting unconscious knowledge and even languages.
    \item Maybe a quick look at the top downloaded R packages on bioconductor and if that correlates with quality documentation?
    \item The quality of hardware and the underlying software dependencies of a package will also change and update, requiring regular maintenance of code for it to be continued to use. However, documentation and reporting the motivations/solutions to problems is often a contribution that last far longer.
    \item Archived and outdated software packages are still being used for the alignment of sequencing data because they are well documented. Simply saying a piece of code is old/out of date is not enough. Justification of the severe negatives of using outdated code should be highlighted in the documentation.
    \item As software get more and more specialised and refined we need to come up with a better way of accrediting work and encouraging documentation so we know exactly how the dependencies run and have a chance of fixing any future eventual bugs. (xkcd meme about software dependency and maintenance. https://xkcd.com/2347/)
\end{itemize}

\subsection{How do we improve research software documentation?}

\begin{itemize}
    \item Lessons learnt from R: vignettes, roxygen, pkgdown
    \item Documentation framework to ensure the right documentation is included and maintainable.
    \item Diataxis is a popular framework for creating documentation along its two axis of knowledge: theory vs practice and acquisition vs application. They separate software documentation into four rough types: Tutorials, practical and ap- plication knowledge; How-tos, practical and acquisition knowledge; references, theoretical and acquisition knowledge; and explaintions, theoretical and application knowledge. %https://diataxis.fr/
    \item User interviews to evaluate documentation as well as functionality
    \item Use of GitHub (issue add transparency to decisions, template issues/pull request help contributors)
    \item Including/understanding the pedagogy of software documentation
    \item Resources available on the Turing Way, open life science and Mozilla
\end{itemize}

% https://agupubs.onlinelibrary.wiley.com/doi/10.1002/2015EA000136 https://f1000research.c 295/v1 https://www.exascaleproject.org/event/softwaredocumentation/ https://elib.dlr.de/147388/
% https://headrush.typepad.com/creatingpassionateusers/2006/01/crashcour 
% https://www.proquest.com/docview/304387282
% https://ieeexplore.ieee.org/document/1241364 https://www.jstor.org/stable/44424301
% https://scholar.lib.vt.edu/ejournals/JOTS/Summer- Fall-2000/holmes.html

\section{CASE STUDY: qPCR Analysis pipeline}

\subsection{Quantitative PCR}

Quantitative PCR is among the most common techniques in biological and biomedical research used for the quantification of DNA and RNA. It is considered the gold standard for nucleotide detection and quantification in medicine, legislation and academia. Its prominence has never more apparent than through its use in curtailing the spread of SARS-CoV-2 with a reliable qPCR assay published within 2 weeks of the first release of the SARS-CoV-2 genome \cite{Li2020}. Nevertheless, the widespread usage of qPCR across distinct disciplines has inhibited the education in and implementation of best-practices and quality control. \cite{Bustin2021} From sample preparation to statistical analysis, each stage of a qPCR experiment can be customised to extract information about numerous biological processes. Developing equally versatile and reliable software built with best-practices and using education by documentation it could enable path to overcome the lack of consistency across the disjoint disciplines. 

The basic principle of a polymerase chain reaction consists of the duplication of a region on DNA or RNA that is specified by two short nucleotide sequences, called primers, that are designed to be complementary to the start and the end of the region of interest. A highly thermal tolerant polymerase, adapted from the bacteria \textit{Thermus aquaticus}, is then able to complete the duplication of the region by elongation of the sequence between the two primers \cite{Holland1991}. The PCR polymerase must be thermal tolerant as the duplication step is rapidly repeated by raising the temperature to melt the newly created complementary stand away from the original strand before dropping the temperature back down to enable the next round of elongation. If this cycle is repeated in a solution with surplus concentrations of primers, enzymes, and nucleotides the increase in copies per cycle is bound by an exponential defined only by the number of transcripts of the target sequence in the original sample. Therefore, if the exponential growth of PCR duplicates can be measured and accurately modelled then the original number of transcripts can be inferred. To measure the exponential duplication, Quantitative PCR introduces dyes that only fluoresce when a region has been successfully duplicated. The fluorescence of the sample is measured as the PCR cycle is repeated to determine the exponential growth in duplicates and quantify the number of transcripts of the target sequence in the original sample.

\subsubsection{qPCR Methods: RNA vs DNA}

\begin{itemize}
    \item qPCR assays are highly optimised for \textit{Thermus aquaticus} polymerase
    \item RNA samples can be analysed with qPCR machines but must be reverse transcribed to cDNA
    \item The addition reverse transcription step is regularly determined to be the source of most variation between RNA samples \cite{Stahlberg2004}
    \item RT priming method, original RNA target concentration and total RNA concentration in the sample can all vary the cDNA yield between replicates.
\end{itemize}

\subsubsection{qPCR Methods: Taqman vs SYBR Green}

# figure outlining different methods


The fluorescent dye used to detect duplicates differs significantly between the two most common qPCR methods: Taqman and SYBR Green. SYBR Green methods contain dye that fluoresce when they bind to any double stranded DNA species in the sample. This leads it to being a cheap and relatively easy to use system, but it is highly susceptible to contamination and it is unable to distinguish between duplicates from different species. TaqMan methods bind the fluorescent dye to a oligonucleotide probe that is designed to attach to the region of interest somewhere between the two primers. In addition to the fluorescent dye on one end the oligonucleotide probe has a quencher on the other. The quencher stops any emissions from the dye from being detected. However, during elongation, when the polymerase reaches the oligonucleotide probe, the probe is hydrolysed separating the dye from the quencher. Emissions from the fluorescent dye can then be measured and the creation of a new duplicate is detected. The introduction of a custom oligonucleotide probe increases the specificity of TaqMan methods and reduces the effects of contamination. Also, the abundance of multiple species can be measured in the same sample by carefully designing different oligonucleotide probes with different fluorescent dyes. Unfortunately, the design and creation of custom probes causes TaqMan methods to be more expensive and technical. Meanwhile, the accuracy of TaqMan methods is comparable to the cheaper SYBR Green methods, if correctly conducted. \cite{Tajadini2014}

\subsubsection{Quantifying Abundance: Curve Fitting vs Cycle Threshold}

The exponential limit of the duplicate number enables two methods that summarise amplification curves to compare abundance across samples. Assuming all samples reach the exponential growth stage in copy number per cycle at the same time then the difference it fluorescence at any cycle of the PCR assay dependent only on the original copy number. Therefore, if you set a particular fluorescence threshold during the exponential phase to compare all samples at, the number of cycles needed to reach that fluorescence level is an accurate proxy for original copy number. A common method to determine target abundance is the number of cycles required to reach a set fluorscence. Unfortunately this method assumes both that each sample reaches the exponential phase at the same time and that each cycle doubles the number of duplicates perfectly for each sample. An alternative method fits an sigmodal curve to the amplification curve and uses this model to deduce the cycles required to reach a the threshold. The additional fitting can account for difference in the times to reach the exponential growth phase between samples and can directly account for deviations in perfect duplication. 

\subsubsection{Quantifying Abundance: Relative vs Absolute}

Multiple methods exist for converting fluorescence measurements into quantitative values for sample abundance whilst accounting for experimental and technical noise. First, qPCR experiments can be designed either to measure the relative change in abundance across samples or estimate the absolute number of copies of a target in a sample. Relative abundance measurements depend on the determination of genes that have constant expression across all samples/conditions. Any change in the gene(s) of interest across samples can then be detected by comparing expressions relative to the set of constantly expressed genes. Normalising the fluorescence to genes with constant expression minimises batch effects introduced by sample preparation, reverse transcription and reactants. Absolute quantification of a target requires a preliminary experiment where known initial quantities of the gene of interest are measured with qPCR. The amplification curve for each sample, with gradually increasing copies of the gene of interest, is measured to create a collection of standard curves. Next, the sample from the primary experiment is measured with qPCR and its amplification curve is compared to the standard curves. The absolute copy number of the gene of interest in the experimental sample can be interpolated.

# table of assumptions for each method

There is a critical need for rigorous analysis and reporting of qPCR experiments, codified in the minimum information for publication of quantitative real-time PCR experiments (MIQE) guidelines \cite{Bustin2009, Bustin2017, Courts2019}. Yet it is common for qPCR to be analysed either by closed-source software supplied by the manufacturers of PCR machines, or by highly variable, in-house analysis scripts that have not been peer-reviewed.



\subsection{Comparing current qPCR analysis software}

Previous reviews \cite{Rodiger2015, Pabinger2014}

Introducing RDML \cite{Rodiger2017}

%Check for software:
%- plate plan
%- version
%- release
%- missing data interpolation
%- normalisation
%- auto-dection of normalising genes
%- absolute or relative quantification
%- plot melt/amp curves
%- standard / efficiency calcs 
%- efficiency in quant calc
%- statistical analysis
%- plotting results
%- support for taqman probes


\textbf{Web Apps}

\textbf{QuantGenius} A standalone web-app for absolute quantification of target abundance with quality control. The analysis pipeline uses standard curves to determine target abundance, checks copy number and summarises multiple QC steps in the final view to aid users in determining significant results. It uses a PHP backend to conduct the analysis.   \cite{Baebler2017}

\textbf{ELIMU-MDx} Web-based elixer tool for the storage and anlysis of clinical qPCR data. Uses RDML to store qpcr machine agnostic metadata and results. Able to deduce relative and absolute target abundance.  \cite{Krahenbuhl2019}

\textbf{shinyCurves} R Shiny web-based app for assay agnostic detection of COVID-19. Accepts excel spreadsheet inputs from qPCR machines and determines if samples are Positive, Negative or Undetermined depending on user defined thresholds. Users can conduct QC too by viewing melt curve plots and amplification curve plots. \cite{OlaecheaLazaro2021}

\textbf{PIPE-T} An extension to the Galaxy web-based bioinformatics platform released in Oct 2019. PIPE-T provides a Galaxy interface for several R based qPCR analysis packages to enable users to chain multiple analysis steps; missing data interpolation, normalisation, and inferring differential expression, without needing to learn to program in R. Flags Cq values with poor quality. It conducts relative quantification but not absolute quantification. Raw Cq values need to be pre-processed so that each sample/replicate/condition is in a separate tab separated text file.  This tool depends heavily on the R packages HTqPCR, NormqPCR described below. \cite{Zanardi2019}

\textbf{SATqPCR} A standalone web-app. Can use primer efficiency in relative quantification count but does not have the functionality to calculate efficiencies from calibration curves. Cannot visualise melt/amp curves. (Rancurel wt al 2019). Raw Cq values need to be pre-processed so that each gene has its own column and the first column has sample name in specific format. Automatically detects most stable genes to use as normalisation genes. Allows t-test and anova statistical tests. Does not detect outliners or missing data interpolation \cite{Rancurel2019}. Based on an R package RqPCRAnalysis (see below).

\textbf{Auto-qPCR} A standalone web-app with Python back-end for the relative and absolute quantification of qPCR data. Users can download the python code to run the app locally. v1 was released in Aug 2020. (Maussion 2021). Enables user-defined multi-reference gene normalisation, outlier identification and up to 2 way anova (for parametric and non-parametric). Does not calculate efficiency or standard curve. \cite{Maussion2021}

\textbf{Chainy} R Shiny web-based app for the relative quantification of targets with efficiency (can calculate efficiency with raw data). Accepts multiple file formats as input: RDML or vender specific outputs. Automatic selection of reference genes (using NormqPCR). Does not remove outliers or conduct missing data interpolation. Can plot amplification and melt curves. Permutation test to fold change significance. \cite{Mallona2017}

\textbf{R Packages}

\textbf{LEMming} A published R script for the normalisation of qPCR data and detection of differential expression without the use of normalising genes. It proposes a linear error model for qPCR experiments and proposes steps to remove several sources of error by assuming the mean $C_t$ values within samples of similarly treated groups are equal. The script primarily depends on BioConductor ExpressionSets, base R linear regression functions and the limma R package. \cite{Feuer2015}

\textbf{pcr} An R package for the quantification of relative transcript abundance. Expects a table of Cq values with each row a different sample as input. Includes functions to conduct statistical tests from t-test to multiple linear regression, normalises to one reference gene. Contains functions to calculate efficiency and the standard curve. \cite{Ahmed2018}

\textbf{HTqPCR} R Bioconductor package for the relative quantification of qPCR data using S4 classes. Input is Cq across genes with each condition contained in separate tab-separated files. User can define outliers of replicates. No automatic reference gene selection but quantile means can be used. Does not plot melt/amp curves. Can test significance with linear models or t-tests. \cite{Dvinge2009}

\textbf{NormqPCR}  R Bioconductor package for the relative quantification of qPCR data using S4 classes. Encapusalates the geNorm function widely used to select reference genes with steady expression. Outliers removed with manual thresholds. Cannot plot melt/amp curves. \cite{Perkins2012}

\textbf{qpcR} An R package for selecting the best sigmoidal model to fit to qPCR data for accurate determination of Cq values and PCR efficiency. It can detect sample outliers. It does not calculate relative or absolute abundances or conduct any significance analysis. \cite{Ritz2008}

\textbf{Misc}

\textbf{Spreadsheet} Published guide for determining relative abundance using spreadsheet software. It provides steps starting from importing qpcr machine data determining significant differential expression. It does not include quality control measure or removal of outliers. \cite{Ng2021}

\subsection{tidyqpcr}

Our package, tidyqpcr, addresses the need for a qPCR analysis package that fully integrates with the user-friendly tidyverse, encourages the use of MIQE best-practice compliant experimental design, and provides detailed example analysis pipelines as R vignettes. Following the tidy data paradigm integrates tidyqpcr into the wider collection of data analysis packages provided by the tidyverse, while being accessible to novice users of R. Other open-source libraries for qPCR analysis are available with distinct aims. HTqPCR [@Dvinge:2009], ReadqPCR/NormqPCR [@Perkins:2012], and qpcR [@Spiess:2018] have similar and in some respects greater functionality than tidyqpcr, but follow object oriented approaches with specialised data objects. By contrast, pcr [@Ahmed:2018] aligns most closely with tidyqpcr but its function inputs are not tidy data frames. Although alternatives are available, tidyqpcr's aims and approach are distinct: to improve the quality of qPCR experiments from plate design to analysis, by exposing all data in a consistent tidy format that integrates with the tidyverse.

\subsubsection{tidyqpcr aims}

\begin{itemize}
    \item Empowering: tidyqpcr combines a free, open-source qPCR analysis R package with online teaching materials.
    \item Reproducible: tidyqpcr scripts produce paper-ready figures straight from raw data with identical results across computers.
    \item Flexible: tidyqpcr follows the 'tidy' data paradigm to ensure scalability and adaptability.
    \item Best-practice compliant: tidyqpcr encourages standardised, reliable experimental design by prioritising MIQE-compliant best practices.
\end{itemize}

\subsubsection{tidyqpcr functionality}

tidyqpcr can be used to analyse qPCR data from any nucleic acid source - DNA for qPCR or ChIP-qPCR, RNA for RT-qPCR. Currently tidyqpcr has functions that explicitly support relative quantification by the $\Delta Cq$ method, but not yet absolute quantification.

\begin{itemize}
    \item use a single data type for analysis as every object is a tibble / data frame.
    \item lay out and display 96/384-well plates for easy experimental setup (label\_plate\_rowcol, create\_blank\_plate, ...).
    \item consistently describe samples and target amplicons with reserved variable names (sample\_id, target\_id).
    \item flexibly assign metadata to samples for visualisation with ggplot2 (see vignettes).
    \item read in quantification cycle (Cq) and raw data from Roche LightCycler machines with single-channel fluorescence (read\_lightcycler\_1colour\_cq, read\_lightcycler\_1colour\_raw).
    \item calibrate primer sets including estimating efficiencies and visualization of curves (calculate\_efficiency).
    \item visualize amplification and melt curves (calculate\_drdt\_plate)
    \item perform normalisation and relative quantification to one or more reference targets by the $\Delta Cq$ method (calculate\_normcq, calculate\_deltacq\_bysampleid).
    \item estimate differential expression across multiple samples by the $\Delta \Delta Cq$ method (calculate\_deltadeltacq\_bytargetid).
    \item accelerate further downstream analysis and visualization by writing tidy data frames that are fully compatible with the tidyverse suite.

\end{itemize}

\subsubsection{User Interviews}
We have conducted a series of user interviews to improve tidyqpcr's capabilities and documentation.

\begin{center}
\begin{tabular}{|| m{5.5cm} | m{8cm} ||} 
 \hline
 \textbf{\large Issue} & \textbf{\large Solution} \\ [0.5ex] 
 \hline\hline
 \multicolumn{2}{|l|}{\textbf{Functionality}} \\
 \hline
 tidyqpcr contains helper functions to create 96 and 384 well plates but 1536 plate wells are not supported. & Created help function to automatically create 1536 plate as well as a function to produce a "pick list" based on the plate to facilitate the use of robotic sample loaders (echo liquid handler). \\ 
 \hline
 \multicolumn{2}{|l|}{\textbf{Usability}} \\
 \hline
 Determining general but intuitive names for function arguments. & Depending on assay used the measurement variable could be called Primer Set (for SYBR dye-style) or a fluorescent-quenched probe (Taqman). Rather than committing to a specific assay we decided on the more general term targetID. \\
 \hline
 \multicolumn{2}{|l|}{\textbf{Documentation}} \\
 \hline
 Current package vignettes overwhelm new users as they introduce the basic concepts of tidyqpcr on multi-condition, multi-target data sets & Interviewee provided a simpler 96-well plate data set for us to use as an example. We created a simpler vignette introducing the basic concepts of tidyqpcr using this data set for users to understand before moving onto the larger example. \\
 \hline
\end{tabular}
\end{center}

\subsubsection{rOpenSci Review}

\begin{itemize}
    \item rOpenSci offers transparent, constructive and open reviews of R packages that lower barriers to working with local and remote scientific data sources.
    \item Successful rOpenSci review also enables submission to JOSS, enabling the software development work to be officially acknowledged with citation.
    \item Improved tidyqpcr's functionality in handling plate edge effects by enabling users to define plates with empty wells and to visualise well Cq values.
    \item Prepared package for submission to CRAN by reducing package size to below 5Mb.
    \item Highlighted issues with accessing internal package data sets and suggested fix.
    \item Enhanced comparison of tidyqpcr functionality with other available packages.
\end{itemize}


\bibliographystyle{apalike}
\bibliography{sbc-template}

\end{document}